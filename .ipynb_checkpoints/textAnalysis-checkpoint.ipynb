{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Medical Questions – Team 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to analyze questions asked by health care providers ('providers') and the general public ('patient') so as to glean insight from both the provider and patient viewpoints.\n",
    "\n",
    "* <a href=\"https://raw.githubusercontent.com/nina-lin/NLTK-analysis/main/data/providers.txt\" target=\"_blank\">provider.txt</a> contains data <a href=\"https://github.com/nina-lin/NLTK-analysis/blob/main/xml-extract.ipynb\" target=\"_blank\">extracted</a> from the <a href=\"https://datadiscovery.nlm.nih.gov/dataset/Clinical-Questions-Collection/i3a4-n4ma\" target=\"_blank\">National Institute of Health's Clinical Questions Collection (1999 - 2003)</a>, a “repository of questions that have been collected between 1991 – 2003 from healthcare providers in clinical settings across the country.”\n",
    "\n",
    "\n",
    "* <a href=\"https://raw.githubusercontent.com/nina-lin/NLTK-analysis/main/data/patients.txt\" target=\"_blank\">patient.txt</a> contains questions of a medical nature asked by the general public from two sources:\n",
    "    1. The <a href=\"https://raw.githubusercontent.com/curai/medical-question-pair-dataset/master/mqp.csv\" target=\"_blank\">Medical Question Pairs (MQP) Dataset</a>, a list of patient-asked questions randomly sampled from a crawl of HealthTap\n",
    "    2. Patient-asked questions <a href=\"https://github.com/nina-lin/NLTK-analysis/blob/main/scraper.ipynb\" target=\"_blank\">scraped</a> from www.thecorrect.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Code snippets adapted from <a href=\"https://github.com/gcrocetti\" target=\"_blank\">Giancarlo Crocetti's</a> CUS635 web scraping materials.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read datasets \n",
    "path = 'data/'  \n",
    "filePrefix = ''\n",
    "categories = ['patients','providers']\n",
    "dataset = {}\n",
    "dataset_raw = {}\n",
    "allFeatures = set()\n",
    "questions = 0\n",
    "question_count = {}\n",
    "\n",
    "N={} # Number of questions in each corpus\n",
    "\n",
    "for category in categories:\n",
    "    fileName = path + filePrefix + category.lower() + '.txt'\n",
    "    f = open(fileName,'r')\n",
    "    text = ''\n",
    "    text_raw = ''    \n",
    "    lines = f.readlines()\n",
    "    questions += len(lines)\n",
    "    question_count[category] = len(lines)\n",
    "    dataset_raw[category] = list(map(lambda line: line.lower(), lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        text += line.replace('\\n',' ').lower()\n",
    "        text_raw = line.lower()\n",
    "    f.close\n",
    "    N[category] = len(lines)\n",
    "    \n",
    "    #create tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    dataset[category] = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the dataset of questions asked by patients have been spellchecked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dataset['providers'])\n",
    "patientsFD = FreqDist(dataset['patients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation & Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"\n",
    "print ('Punctuation FD[providers] FD[patients]')\n",
    "for punct in punctuations:\n",
    "    print ('   {}  {:3d}   {:3d}'.format(punct,providersFD[punct], patientsFD[punct]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "dsCleaned = {} #cleaned dataset\n",
    "\n",
    "def removePunctuation(corpus):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filteredCorpus = [token for token in corpus if (not token in punctuations)]\n",
    "    return filteredCorpus\n",
    "\n",
    "# #note that token.decode('utf-8') was dropped for Python 3. decode ('utf-8') will throw an error. Please use the below snippet only with Python 2.\n",
    "# def apply_stopwording(corpus, min_len):\n",
    "#     filtered_corpus = [token.decode('utf-8') for token in corpus if (not token.decode('utf-8') in stopwords.words('english') and len(token)>min_len)]\n",
    "#     return filtered_corpus\n",
    "\n",
    "def stopwording(corpus, min_len):\n",
    "    filteredCorpus = [token for token in corpus if (not token in stopwords.words('english') and len(token) > min_len)]\n",
    "    return filteredCorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove punct & apply stopwording\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dsCleaned[category] = stopwording(removePunctuation(dataset[category]), 3)\n",
    "    print (dsCleaned[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "dsFinal={} #working dataset (final)\n",
    "\n",
    "# def stemming(corpus):\n",
    "#     stemmer = nltk.PorterStemmer()\n",
    "#     normalized_corpus = [stemmer.stem(token) for token in corpus]\n",
    "#     return normalized_corpus\n",
    "\n",
    "def lemmatization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    normalized_corpus = [lemmatizer.lemmatize(token) for token in corpus]\n",
    "    return normalized_corpus\n",
    "\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dsFinal[category] = lemmatization(dsCleaned[category])\n",
    "    print (dsFinal[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for category in categories: \n",
    "    vocabulary[category] = sorted(set(dsFinal[category]))\n",
    "    print ('vocabulary for %s = [%s]' % (category, vocabulary[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Patient questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['patients']), len(vocabulary['patients'])))\n",
    "print ('Provider questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['providers']), len(vocabulary['providers'])))\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(text)*1.0/len(set(text))\n",
    "\n",
    "lexDiversity = {}\n",
    "for category in categories:\n",
    "    lexDiversity[category] = lexical_diversity(dsFinal[category])\n",
    "    print ('Lexical Diversity in %s = %s' % (category,lexDiversity[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('# time \"disease\" is used by health care providers %s' % ds['providers'].count('disease'))\n",
    "# print ('# time \"disease\" is used by patients %s' % ds['patients'].count('disease'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "# for token in vocabulary[category]:\n",
    "#     count[token] = dsFinal[category].count(token)\n",
    "\n",
    "for token in vocabulary['providers']:\n",
    "    count[token] = dsFinal['providers'].count(token)\n",
    "    \n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "for token in vocabulary['patients']:\n",
    "    count[token] = dsFinal['patients'].count(token)\n",
    "\n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequent(freq, n):\n",
    "    result = {}\n",
    "    index = 0\n",
    "    for i in sorted(freq, key = freq.get, reverse = True):\n",
    "        index += 1\n",
    "        result[i] = freq[i]\n",
    "        if index == n:\n",
    "            break\n",
    "    return result \n",
    "\n",
    "frequency = nltk.FreqDist(dsFinal[category])\n",
    "\n",
    "topTokens = getFrequent(frequency, 50)\n",
    "\n",
    "print(topTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dsFinal['providers'])\n",
    "patientsFD = FreqDist(dsFinal['patients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FreqDist(dsFinal[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "providersFD.plot(20, cumulative = False, title = 'Provider Tokens');\n",
    "patientsFD.plot(20, cumulative = False, title = 'Patient Tokens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract important words and phrases, filtering out words that are shorter than 5 characters in length to ignore preopositions, determiners, and interrogative words (\"who\", \"what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dsFinal['patients']:\n",
    "    if (len(token) >= 5):\n",
    "        print ('%s [%s]' % (token, patientsFD[token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dsFinal['providers']:\n",
    "    if (len(token) >= 5):\n",
    "        print ('%s [%s]' % (token, patientsFD[token]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations, 2-grams & Co-Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['patients']\n",
    "dsFinal['patients'] = nltk.Text(dsFinal['patients'])\n",
    "dsFinal['patients'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['providers']\n",
    "dsFinal['providers'] = nltk.Text(dsFinal['providers'])\n",
    "dsFinal['providers'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "\n",
    "print ('Generating bigrams')\n",
    "bigrams = ngrams(dsFinal['patients'],2)\n",
    "for bigram in bigrams:\n",
    "    print (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Generating bigrams')\n",
    "bigrams = ngrams(dsFinal['providers'],2)\n",
    "for bigram in bigrams:\n",
    "    print (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "trigram = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Finding frequent 2-grams\n",
    "print ('Finding frequent 2-grams')\n",
    "for category in categories:\n",
    "    finder = BigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(bigram.pmi, 20)\n",
    "    print (tokens)\n",
    "    \n",
    "# Finding frequent 3-grams\n",
    "print ('\\nFinding frequent 3-grams')\n",
    "for category in categories:\n",
    "    finder = TrigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(trigram.pmi, 20)\n",
    "    print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_cleantext(corpus, language):\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    cleantext = [token for token in corpus if token not in stopwords]\n",
    "    return len(cleantext)*1.0/len(corpus)\n",
    "\n",
    "language='english'\n",
    "for category in categories:\n",
    "    print (\"Proportion of clean terms in the [%s] is %s\" % (category,proportion_cleantext(dataset[category],language)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all functions needed for the implementation of the Naive Bayes classifier\n",
    "\n",
    "<em>(Note that \n",
    "\n",
    "* tot_articles = questions \n",
    "* articles_count = question_count\n",
    "\n",
    "as adapted from <strong>Improving Classification of Web Articles's</strong> NB classifier for our own use)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = {}\n",
    "category_count = {}\n",
    "probCat = {}\n",
    "\n",
    "# Calculate the probabilities for each category\n",
    "for category in categories:\n",
    "    probCat[category] = question_count[category] * 1.0 / questions\n",
    "    print (\"%s - p(%s)=%s\" % (category, category, probCat[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqWord = {}\n",
    "wordCounts = {}\n",
    "\n",
    "def buildFrequencies(dsFinal):\n",
    "    for category in categories:\n",
    "        freqWord[category] = FreqDist(dsFinal[category])\n",
    "        wordCounts[category] = len(dsFinal[category])\n",
    "        \n",
    "buildFrequencies(dsFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleWord = 'blood'\n",
    "\n",
    "print (f\"Checking Frequencies for the word '{sampleWord}':\")\n",
    "print (\"F('blood'|'providers') = %s\" % freqWord['providers'][sampleWord])\n",
    "print (\"F('blood'|'patients') = %s\" % freqWord['patients'][sampleWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
