{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Medical Questions – Team 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to analyze questions asked by health care providers ('providers') and the general public ('patient') so as to glean insight from both the provider and patient viewpoints.\n",
    "\n",
    "* <a href=\"https://raw.githubusercontent.com/nina-lin/NLTK-analysis/main/data/providers.txt\" target=\"_blank\">provider.txt</a> contains data <a href=\"https://github.com/nina-lin/NLTK-analysis/blob/main/xml-extract.ipynb\" target=\"_blank\">extracted</a> from the <a href=\"https://datadiscovery.nlm.nih.gov/dataset/Clinical-Questions-Collection/i3a4-n4ma\" target=\"_blank\">National Institute of Health's Clinical Questions Collection (1999 - 2003)</a>, a “repository of questions that have been collected between 1991 – 2003 from healthcare providers in clinical settings across the country.”\n",
    "\n",
    "\n",
    "* <a href=\"https://raw.githubusercontent.com/nina-lin/NLTK-analysis/main/data/patients.txt\" target=\"_blank\">patient.txt</a> contains questions of a medical nature asked by the general public from two sources:\n",
    "    1. The <a href=\"https://raw.githubusercontent.com/curai/medical-question-pair-dataset/master/mqp.csv\" target=\"_blank\">Medical Question Pairs (MQP) Dataset</a>, a list of patient-asked questions randomly sampled from a crawl of HealthTap\n",
    "    2. Patient-asked questions <a href=\"https://github.com/nina-lin/NLTK-analysis/blob/main/scraper.ipynb\" target=\"_blank\">scraped</a> from www.thecorrect.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Code snippets adapted from <a href=\"https://github.com/gcrocetti\" target=\"_blank\">Giancarlo Crocetti's</a> CUS635 web scraping materials.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, questions have been manually spell-checked for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read datasets \n",
    "path = 'data/'  \n",
    "filePrefix = ''\n",
    "categories = ['patients','providers']\n",
    "dataset = {}\n",
    "dataset_raw = {}\n",
    "allFeatures = set()\n",
    "questions = 0\n",
    "question_count = {}\n",
    "\n",
    "corpus = []\n",
    "text = \"\"\n",
    "\n",
    "N={} # Number of questions in each corpus\n",
    "\n",
    "for category in categories:\n",
    "    fileName = path + filePrefix + category.lower() + '.txt'\n",
    "    f = open(fileName,'r')\n",
    "    text = ''\n",
    "    text_raw = ''    \n",
    "    lines = f.readlines()\n",
    "    questions += len(lines)\n",
    "    question_count[category] = len(lines)\n",
    "    dataset_raw[category] = list(map(lambda line: line.lower(), lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        text += line.replace('\\n',' ').lower()\n",
    "        text_raw = line.lower()\n",
    "    f.close\n",
    "    N[category] = len(lines)\n",
    "    \n",
    "    #create tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    dataset[category] = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation & Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dataset['providers'])\n",
    "patientsFD = FreqDist(dataset['patients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation FD[providers] FD[patients]\n",
      "   .  5169   2055\n",
      "   ,  2767   2711\n",
      "   \"    0     0\n",
      "   -   24    12\n",
      "   \\    0     0\n",
      "   /    0     5\n",
      "   #    2     0\n",
      "   !    4    46\n",
      "   ?  6038   3320\n",
      "   $    4     0\n",
      "   %   49    13\n",
      "   \\    0     0\n",
      "   ^    0     0\n",
      "   &    0    53\n",
      "   \\    0     0\n",
      "   *    1     3\n",
      "   ;   17    13\n",
      "   :   83    22\n",
      "   {    0     0\n",
      "   }    0     0\n",
      "   =    8     0\n",
      "   \\    0     0\n",
      "   -   24    12\n",
      "   _    0     0\n",
      "   '   28    48\n",
      "   ~    0     0\n",
      "   (  1656   174\n",
      "   )  1652   169\n"
     ]
    }
   ],
   "source": [
    "punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"\n",
    "print ('Punctuation FD[providers] FD[patients]')\n",
    "for punct in punctuations:\n",
    "    print ('   {}  {:3d}   {:3d}'.format(punct,providersFD[punct], patientsFD[punct]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "dsCleaned = {} #cleaned dataset\n",
    "\n",
    "def removePunctuation(corpus):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filteredCorpus = [token for token in corpus if (not token in punctuations)]\n",
    "    return filteredCorpus\n",
    "\n",
    "def stopwording(corpus, min_len):\n",
    "    filteredCorpus = [token for token in corpus if (not token in stopwords.words('english') and len(token) > min_len)]\n",
    "    return filteredCorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patients\n"
     ]
    }
   ],
   "source": [
    "#remove punct & apply stopwording\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dsCleaned[category] = stopwording(removePunctuation(dataset[category]), 3)\n",
    "    print (dsCleaned[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "dsFinal={} #working dataset (final)\n",
    "\n",
    "# def stemming(corpus):\n",
    "#     stemmer = nltk.PorterStemmer()\n",
    "#     normalized_corpus = [stemmer.stem(token) for token in corpus]\n",
    "#     return normalized_corpus\n",
    "\n",
    "def lemmatization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    normalized_corpus = [lemmatizer.lemmatize(token) for token in corpus]\n",
    "    return normalized_corpus\n",
    "\n",
    "# for category in categories:\n",
    "#     print ('Processing %s' % category)\n",
    "#     dsFinal[category] = lemmatization(dsCleaned[category])\n",
    "#     print (dsFinal[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for category in categories: \n",
    "    vocabulary[category] = sorted(set(dsFinal[category]))\n",
    "    print ('vocabulary for %s = [%s]' % (category, vocabulary[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Patient questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['patients']), len(vocabulary['patients'])))\n",
    "print ('Provider questions have a total of %s tokens and a vocabulary size of %s' % (len(dsFinal['providers']), len(vocabulary['providers'])))\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(text)*1.0/len(set(text))\n",
    "\n",
    "lexDiversity = {}\n",
    "for category in categories:\n",
    "    lexDiversity[category] = lexical_diversity(dsFinal[category])\n",
    "    print ('Lexical Diversity in %s = %s' % (category,lexDiversity[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('# time \"disease\" is used by health care providers %s' % ds['providers'].count('disease'))\n",
    "# print ('# time \"disease\" is used by patients %s' % ds['patients'].count('disease'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "# for token in vocabulary[category]:\n",
    "#     count[token] = dsFinal[category].count(token)\n",
    "\n",
    "for token in vocabulary['providers']:\n",
    "    count[token] = dsFinal['providers'].count(token)\n",
    "    \n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "for token in vocabulary['patients']:\n",
    "    count[token] = dsFinal['patients'].count(token)\n",
    "\n",
    "for w in sorted(count, key = count.get, reverse=True):\n",
    "    print (w, count[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequent(freq, n):\n",
    "    result = {}\n",
    "    index = 0\n",
    "    for i in sorted(freq, key = freq.get, reverse = True):\n",
    "        index += 1\n",
    "        result[i] = freq[i]\n",
    "        if index == n:\n",
    "            break\n",
    "    return result \n",
    "\n",
    "frequency = nltk.FreqDist(dsFinal[category])\n",
    "\n",
    "topTokens = getFrequent(frequency, 50)\n",
    "\n",
    "print(topTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "providersFD = FreqDist(dsFinal['providers'])\n",
    "patientsFD = FreqDist(dsFinal['patients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FreqDist(dsFinal[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "providersFD.plot(20, cumulative = False, title = 'Provider Tokens');\n",
    "patientsFD.plot(20, cumulative = False, title = 'Patient Tokens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract important words and phrases, filtering out words that are shorter than 5 characters in length to ignore preopositions, determiners, and interrogative words (\"who\", \"what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dsFinal['patients']:\n",
    "    if (len(token) >= 5):\n",
    "        print ('%s [%s]' % (token, patientsFD[token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dsFinal['providers']:\n",
    "    if (len(token) >= 5):\n",
    "        print ('%s [%s]' % (token, patientsFD[token]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations, 2-grams & Co-Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['patients']\n",
    "dsFinal['patients'] = nltk.Text(dsFinal['patients'])\n",
    "dsFinal['patients'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFinal['providers']\n",
    "dsFinal['providers'] = nltk.Text(dsFinal['providers'])\n",
    "dsFinal['providers'].collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "\n",
    "print ('Generating bigrams')\n",
    "bigrams = ngrams(dsFinal['patients'],2)\n",
    "for bigram in bigrams:\n",
    "    print (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Generating bigrams')\n",
    "bigrams = ngrams(dsFinal['providers'],2)\n",
    "for bigram in bigrams:\n",
    "    print (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "trigram = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Finding frequent 2-grams\n",
    "print ('Finding frequent 2-grams')\n",
    "for category in categories:\n",
    "    finder = BigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(bigram.pmi, 20)\n",
    "    print (tokens)\n",
    "    \n",
    "# Finding frequent 3-grams\n",
    "print ('\\nFinding frequent 3-grams')\n",
    "for category in categories:\n",
    "    finder = TrigramCollocationFinder.from_words(dsFinal[category])\n",
    "    finder.apply_freq_filter(5)\n",
    "    tokens = finder.nbest(trigram.pmi, 20)\n",
    "    print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_cleantext(corpus, language):\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    cleantext = [token for token in corpus if token not in stopwords]\n",
    "    return len(cleantext)*1.0/len(corpus)\n",
    "\n",
    "language='english'\n",
    "for category in categories:\n",
    "    print (\"Proportion of clean terms in the [%s] is %s\" % (category,proportion_cleantext(dataset[category],language)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topical Discovery and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify the overall topic of discussion\n",
    "2. Identify topics brought forward by Patients\n",
    "3. Identify topics brought forward by Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore depreciation warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.collocations import *\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_utf(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r' ', text)\n",
    "\n",
    "def load_stopwords():\n",
    "    swords = []\n",
    "    path=\"data/stopwords.txt\"\n",
    "    file_input = open (path,\"r\")\n",
    "    lines = file_input.readlines()\n",
    "    for line in lines:\n",
    "        swords.append(line[:-1])\n",
    "    file_input.close()\n",
    "    return swords\n",
    "\n",
    "def loadCorpus(path):\n",
    "    data = []\n",
    "    file_input = open (path,\"r\")\n",
    "    lines = file_input.readlines()\n",
    "    for line in lines:\n",
    "        data.append(remove_utf(line[:-1].lower()))\n",
    "    file_input.close()\n",
    "    return data\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "path = \"data/splitcombo.txt\"\n",
    "allQuestions = loadCorpus(path)\n",
    "#print (allQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsAsked = {'patients:':[],'providers:':[]}\n",
    "\n",
    "keys = questionsAsked.keys()\n",
    "\n",
    "current = \"\"\n",
    "for line in allQuestions:\n",
    "    if len(line)>5:\n",
    "        for key in keys:\n",
    "            if line.startswith(key):\n",
    "                current = key\n",
    "        l = questionsAsked[current]\n",
    "        l.append(line)\n",
    "        questionsAsked[current]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More preprocessing for LDA topic modeling (builds on the cleaning, stopwording and lemmatization steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollocations(text, min_freq, coll_num):\n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(text)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    collocations = finder.nbest(bigrams.pmi, coll_num)\n",
    "    return collocations\n",
    "\n",
    "def replaceCollocationsInText(text,collocations):\n",
    "    first = [t[0]for t in collocations]\n",
    "    second = [t[1] for t in collocations]\n",
    "\n",
    "    dtokens = []\n",
    "    i = 0\n",
    "    while i<=(len(text)-1):\n",
    "        try:\n",
    "            idx1 = first.index(text[i])\n",
    "            if (text[i+1]==second[idx1]):\n",
    "                dtokens.append(first[idx1]+\"_\"+second[idx1])\n",
    "                i=i+1\n",
    "        except:\n",
    "            dtokens.append(text[i])\n",
    "            pass\n",
    "        i=i+1\n",
    "    return dtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpusData):\n",
    "    min_frequency = 3\n",
    "    num_of_collocations=100\n",
    "    text=\"\"\n",
    "    corpus=[]\n",
    "    token =[]\n",
    "    \n",
    "    #Extract corpus and preprocess data\n",
    "    for line in corpusData:\n",
    "        t = nltk.word_tokenize(line)\n",
    "        doc = nltk.Text(t)\n",
    "        doc_clean = nltk.Text(lemmatization(stopwording(removePunctuation(doc), 3)))\n",
    "        corpus.append(doc_clean)\n",
    "        token.extend(doc_clean.tokens)\n",
    "        text=text+line\n",
    "    \n",
    "    #Identify collocations\n",
    "    collocations = getCollocations(tokens,min_frequency,num_of_collocations)\n",
    "    docs = []\n",
    "    for doc in corpus:\n",
    "        t = replaceCollocationsInText(doc,collocations)\n",
    "        if (len(t)>0):\n",
    "            docs.append(replaceCollocationsInText(doc,collocations))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = processCorpus(allQuestions)\n",
    "#print(len(docs))\n",
    "#print (docs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag-of-words representation of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "iterations = 40\n",
    "\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualizing Topics by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vizualizing provider topics \n",
    "\n",
    "k = 3\n",
    "\n",
    "docs = processCorpus(questionsAsked[\"providers:\"])\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vizualizing patient topics\n",
    "\n",
    "k = 3\n",
    "\n",
    "docs = processCorpus(questionsAsked[\"patients:\"])\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "topic_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word = dictionary, passes = iterations)\n",
    "lda_vis = pyLDAvis.gensim_models.prepare(topic_model,corpus,dictionary,sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
